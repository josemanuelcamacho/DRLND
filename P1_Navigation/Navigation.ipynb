{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:Failed to extract font properties from /System/Library/Fonts/Apple Color Emoji.ttc: In FT2Font: Could not set the fontsize (error code 0x17)\n",
      "INFO:matplotlib.font_manager:Failed to extract font properties from /Library/Fonts/NISC18030.ttf: In FT2Font: Could not set the fontsize (error code 0x17)\n",
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "from dqn_agent import Agent\n",
    "from collections import deque\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.93\n",
      "Episode 200\tAverage Score: 3.17\n",
      "Episode 300\tAverage Score: 5.75\n",
      "Episode 400\tAverage Score: 8.80\n",
      "Episode 500\tAverage Score: 12.14\n",
      "Episode 600\tAverage Score: 12.23\n",
      "Episode 700\tAverage Score: 12.51\n",
      "Episode 752\tAverage Score: 13.00\n",
      "Environment solved in 652 episodes!\tAverage Score: 13.00\tLast max score: 13.00\n",
      "Episode 753\tAverage Score: 13.04\n",
      "Environment solved in 653 episodes!\tAverage Score: 13.04\tLast max score: 13.04\n",
      "Episode 754\tAverage Score: 13.06\n",
      "Environment solved in 654 episodes!\tAverage Score: 13.06\tLast max score: 13.06\n",
      "Episode 755\tAverage Score: 13.09\n",
      "Environment solved in 655 episodes!\tAverage Score: 13.09\tLast max score: 13.09\n",
      "Episode 756\tAverage Score: 13.09\n",
      "Environment solved in 656 episodes!\tAverage Score: 13.09\tLast max score: 13.09\n",
      "Episode 757\tAverage Score: 13.21\n",
      "Environment solved in 657 episodes!\tAverage Score: 13.21\tLast max score: 13.21\n",
      "Episode 758\tAverage Score: 13.31\n",
      "Environment solved in 658 episodes!\tAverage Score: 13.31\tLast max score: 13.31\n",
      "Episode 760\tAverage Score: 13.32\n",
      "Environment solved in 660 episodes!\tAverage Score: 13.32\tLast max score: 13.32\n",
      "Episode 761\tAverage Score: 13.36\n",
      "Environment solved in 661 episodes!\tAverage Score: 13.36\tLast max score: 13.36\n",
      "Episode 762\tAverage Score: 13.40\n",
      "Environment solved in 662 episodes!\tAverage Score: 13.40\tLast max score: 13.40\n",
      "Episode 770\tAverage Score: 13.42\n",
      "Environment solved in 670 episodes!\tAverage Score: 13.42\tLast max score: 13.42\n",
      "Episode 797\tAverage Score: 13.42\n",
      "Environment solved in 697 episodes!\tAverage Score: 13.42\tLast max score: 13.42\n",
      "Episode 798\tAverage Score: 13.48\n",
      "Environment solved in 698 episodes!\tAverage Score: 13.48\tLast max score: 13.48\n",
      "Episode 800\tAverage Score: 13.49\n",
      "\n",
      "Environment solved in 700 episodes!\tAverage Score: 13.49\tLast max score: 13.49\n",
      "Episode 801\tAverage Score: 13.50\n",
      "Environment solved in 701 episodes!\tAverage Score: 13.50\tLast max score: 13.50\n",
      "Episode 819\tAverage Score: 13.56\n",
      "Environment solved in 719 episodes!\tAverage Score: 13.56\tLast max score: 13.56\n",
      "Episode 820\tAverage Score: 13.58\n",
      "Environment solved in 720 episodes!\tAverage Score: 13.58\tLast max score: 13.58\n",
      "Episode 823\tAverage Score: 13.63\n",
      "Environment solved in 723 episodes!\tAverage Score: 13.63\tLast max score: 13.63\n",
      "Episode 824\tAverage Score: 13.68\n",
      "Environment solved in 724 episodes!\tAverage Score: 13.68\tLast max score: 13.68\n",
      "Episode 831\tAverage Score: 13.75\n",
      "Environment solved in 731 episodes!\tAverage Score: 13.75\tLast max score: 13.75\n",
      "Episode 832\tAverage Score: 13.81\n",
      "Environment solved in 732 episodes!\tAverage Score: 13.81\tLast max score: 13.81\n",
      "Episode 833\tAverage Score: 13.85\n",
      "Environment solved in 733 episodes!\tAverage Score: 13.85\tLast max score: 13.85\n",
      "Episode 834\tAverage Score: 13.85\n",
      "Environment solved in 734 episodes!\tAverage Score: 13.85\tLast max score: 13.85\n",
      "Episode 874\tAverage Score: 13.85\n",
      "Environment solved in 774 episodes!\tAverage Score: 13.85\tLast max score: 13.85\n",
      "Episode 875\tAverage Score: 13.85\n",
      "Environment solved in 775 episodes!\tAverage Score: 13.85\tLast max score: 13.85\n",
      "Episode 876\tAverage Score: 13.94\n",
      "Environment solved in 776 episodes!\tAverage Score: 13.94\tLast max score: 13.94\n",
      "Episode 877\tAverage Score: 14.02\n",
      "Environment solved in 777 episodes!\tAverage Score: 14.02\tLast max score: 14.02\n",
      "Episode 878\tAverage Score: 14.05\n",
      "Environment solved in 778 episodes!\tAverage Score: 14.05\tLast max score: 14.05\n",
      "Episode 881\tAverage Score: 14.12\n",
      "Environment solved in 781 episodes!\tAverage Score: 14.12\tLast max score: 14.12\n",
      "Episode 882\tAverage Score: 14.17\n",
      "Environment solved in 782 episodes!\tAverage Score: 14.17\tLast max score: 14.17\n",
      "Episode 883\tAverage Score: 14.25\n",
      "Environment solved in 783 episodes!\tAverage Score: 14.25\tLast max score: 14.25\n",
      "Episode 884\tAverage Score: 14.28\n",
      "Environment solved in 784 episodes!\tAverage Score: 14.28\tLast max score: 14.28\n",
      "Episode 885\tAverage Score: 14.29\n",
      "Environment solved in 785 episodes!\tAverage Score: 14.29\tLast max score: 14.29\n",
      "Episode 888\tAverage Score: 14.30\n",
      "Environment solved in 788 episodes!\tAverage Score: 14.30\tLast max score: 14.30\n",
      "Episode 897\tAverage Score: 14.31\n",
      "Environment solved in 797 episodes!\tAverage Score: 14.31\tLast max score: 14.31\n",
      "Episode 899\tAverage Score: 14.32\n",
      "Environment solved in 799 episodes!\tAverage Score: 14.32\tLast max score: 14.32\n",
      "Episode 900\tAverage Score: 14.31\n",
      "Episode 901\tAverage Score: 14.35\n",
      "Environment solved in 801 episodes!\tAverage Score: 14.35\tLast max score: 14.35\n",
      "Episode 903\tAverage Score: 14.46\n",
      "Environment solved in 803 episodes!\tAverage Score: 14.46\tLast max score: 14.46\n",
      "Episode 904\tAverage Score: 14.51\n",
      "Environment solved in 804 episodes!\tAverage Score: 14.51\tLast max score: 14.51\n",
      "Episode 905\tAverage Score: 14.52\n",
      "Environment solved in 805 episodes!\tAverage Score: 14.52\tLast max score: 14.52\n",
      "Episode 906\tAverage Score: 14.62\n",
      "Environment solved in 806 episodes!\tAverage Score: 14.62\tLast max score: 14.62\n",
      "Episode 910\tAverage Score: 14.70\n",
      "Environment solved in 810 episodes!\tAverage Score: 14.70\tLast max score: 14.70\n",
      "Episode 914\tAverage Score: 14.77\n",
      "Environment solved in 814 episodes!\tAverage Score: 14.77\tLast max score: 14.77\n",
      "Episode 915\tAverage Score: 14.81\n",
      "Environment solved in 815 episodes!\tAverage Score: 14.81\tLast max score: 14.81\n",
      "Episode 921\tAverage Score: 14.83\n",
      "Environment solved in 821 episodes!\tAverage Score: 14.83\tLast max score: 14.83\n",
      "Episode 923\tAverage Score: 14.83\n",
      "Environment solved in 823 episodes!\tAverage Score: 14.83\tLast max score: 14.83\n",
      "Episode 928\tAverage Score: 14.87\n",
      "Environment solved in 828 episodes!\tAverage Score: 14.87\tLast max score: 14.87\n",
      "Episode 929\tAverage Score: 14.93\n",
      "Environment solved in 829 episodes!\tAverage Score: 14.93\tLast max score: 14.93\n",
      "Episode 930\tAverage Score: 14.96\n",
      "Environment solved in 830 episodes!\tAverage Score: 14.96\tLast max score: 14.96\n",
      "Episode 937\tAverage Score: 15.01\n",
      "Environment solved in 837 episodes!\tAverage Score: 15.01\tLast max score: 15.01\n",
      "Episode 938\tAverage Score: 15.03\n",
      "Environment solved in 838 episodes!\tAverage Score: 15.03\tLast max score: 15.03\n",
      "Episode 939\tAverage Score: 15.08\n",
      "Environment solved in 839 episodes!\tAverage Score: 15.08\tLast max score: 15.08\n",
      "Episode 943\tAverage Score: 15.12\n",
      "Environment solved in 843 episodes!\tAverage Score: 15.12\tLast max score: 15.12\n",
      "Episode 944\tAverage Score: 15.14\n",
      "Environment solved in 844 episodes!\tAverage Score: 15.14\tLast max score: 15.14\n",
      "Episode 945\tAverage Score: 15.18\n",
      "Environment solved in 845 episodes!\tAverage Score: 15.18\tLast max score: 15.18\n",
      "Episode 946\tAverage Score: 15.26\n",
      "Environment solved in 846 episodes!\tAverage Score: 15.26\tLast max score: 15.26\n",
      "Episode 947\tAverage Score: 15.36\n",
      "Environment solved in 847 episodes!\tAverage Score: 15.36\tLast max score: 15.36\n",
      "Episode 948\tAverage Score: 15.37\n",
      "Environment solved in 848 episodes!\tAverage Score: 15.37\tLast max score: 15.37\n",
      "Episode 949\tAverage Score: 15.48\n",
      "Environment solved in 849 episodes!\tAverage Score: 15.48\tLast max score: 15.48\n",
      "Episode 1000\tAverage Score: 14.86\n",
      "Episode 1080\tAverage Score: 15.49\n",
      "Environment solved in 980 episodes!\tAverage Score: 15.49\tLast max score: 15.49\n",
      "Episode 1081\tAverage Score: 15.57\n",
      "Environment solved in 981 episodes!\tAverage Score: 15.57\tLast max score: 15.57\n",
      "Episode 1082\tAverage Score: 15.61\n",
      "Environment solved in 982 episodes!\tAverage Score: 15.61\tLast max score: 15.61\n",
      "Episode 1084\tAverage Score: 15.62\n",
      "Environment solved in 984 episodes!\tAverage Score: 15.62\tLast max score: 15.62\n",
      "Episode 1085\tAverage Score: 15.63\n",
      "Environment solved in 985 episodes!\tAverage Score: 15.63\tLast max score: 15.63\n",
      "Episode 1086\tAverage Score: 15.67\n",
      "Environment solved in 986 episodes!\tAverage Score: 15.67\tLast max score: 15.67\n",
      "Episode 1100\tAverage Score: 15.63\n",
      "Episode 1103\tAverage Score: 15.68\n",
      "Environment solved in 1003 episodes!\tAverage Score: 15.68\tLast max score: 15.68\n",
      "Episode 1107\tAverage Score: 15.68\n",
      "Environment solved in 1007 episodes!\tAverage Score: 15.68\tLast max score: 15.68\n",
      "Episode 1200\tAverage Score: 14.60\n",
      "Episode 1300\tAverage Score: 15.32\n",
      "Episode 1387\tAverage Score: 15.70\n",
      "Environment solved in 1287 episodes!\tAverage Score: 15.70\tLast max score: 15.70\n",
      "Episode 1388\tAverage Score: 15.70\n",
      "Environment solved in 1288 episodes!\tAverage Score: 15.70\tLast max score: 15.70\n",
      "Episode 1389\tAverage Score: 15.72\n",
      "Environment solved in 1289 episodes!\tAverage Score: 15.72\tLast max score: 15.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1390\tAverage Score: 15.72\n",
      "Environment solved in 1290 episodes!\tAverage Score: 15.72\tLast max score: 15.72\n",
      "Episode 1391\tAverage Score: 15.73\n",
      "Environment solved in 1291 episodes!\tAverage Score: 15.73\tLast max score: 15.73\n",
      "Episode 1392\tAverage Score: 15.74\n",
      "Environment solved in 1292 episodes!\tAverage Score: 15.74\tLast max score: 15.74\n",
      "Episode 1394\tAverage Score: 15.79\n",
      "Environment solved in 1294 episodes!\tAverage Score: 15.79\tLast max score: 15.79\n",
      "Episode 1395\tAverage Score: 15.79\n",
      "Environment solved in 1295 episodes!\tAverage Score: 15.79\tLast max score: 15.79\n",
      "Episode 1396\tAverage Score: 15.82\n",
      "Environment solved in 1296 episodes!\tAverage Score: 15.82\tLast max score: 15.82\n",
      "Episode 1399\tAverage Score: 15.82\n",
      "Environment solved in 1299 episodes!\tAverage Score: 15.82\tLast max score: 15.82\n",
      "Episode 1400\tAverage Score: 15.81\n",
      "Episode 1401\tAverage Score: 15.88\n",
      "Environment solved in 1301 episodes!\tAverage Score: 15.88\tLast max score: 15.88\n",
      "Episode 1402\tAverage Score: 15.89\n",
      "Environment solved in 1302 episodes!\tAverage Score: 15.89\tLast max score: 15.89\n",
      "Episode 1403\tAverage Score: 15.89\n",
      "Environment solved in 1303 episodes!\tAverage Score: 15.89\tLast max score: 15.89\n",
      "Episode 1404\tAverage Score: 15.93\n",
      "Environment solved in 1304 episodes!\tAverage Score: 15.93\tLast max score: 15.93\n",
      "Episode 1408\tAverage Score: 15.94\n",
      "Environment solved in 1308 episodes!\tAverage Score: 15.94\tLast max score: 15.94\n",
      "Episode 1409\tAverage Score: 15.94\n",
      "Environment solved in 1309 episodes!\tAverage Score: 15.94\tLast max score: 15.94\n",
      "Episode 1500\tAverage Score: 14.78\n",
      "Episode 1600\tAverage Score: 14.99\n",
      "Episode 1700\tAverage Score: 14.89\n",
      "Episode 1800\tAverage Score: 15.19\n",
      "Episode 1900\tAverage Score: 14.74\n",
      "Episode 2000\tAverage Score: 14.39\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-973a0828cd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# plot the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    last_max_score = 0.0\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0                                          # initialize the score\n",
    "        #for t in range(max_t):\n",
    "        while True:\n",
    "            action = agent.act(state, eps)                 # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward                                # update the score\n",
    "            state = next_state                             # roll over the state to next time step\n",
    "            if done:                                       # exit loop if episode finished\n",
    "                break\n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0 and np.mean(scores_window)>=last_max_score:\n",
    "            last_max_score = np.mean(scores_window)\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}\\tLast max score: {:.2f}'.format(i_episode-100, np.mean(scores_window), last_max_score))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_'+str(np.mean(scores_window))+'.pth')\n",
    "            #break\n",
    "    return scores\n",
    "\n",
    "retrain = False\n",
    "if retrain:\n",
    "    scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint_15.94.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 10.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "eps = 0.01\n",
    "\n",
    "while True:\n",
    "    action = agent.act(state, eps)                 # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
